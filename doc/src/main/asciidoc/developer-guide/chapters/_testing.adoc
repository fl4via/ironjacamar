[[_testing]]
= Testing

[[_testinggoals]]
== Overall goals


The overall goals of our test environment is to execute tests that ensure that we have full coverage of the JCA specification as well as our implementation.

The full test suite runs every time you build the project.

[source]
----
./gradlew --info
----

You can run just testsuite w/o remake a full build
[source]
---
./gradlew test --info
---

There is few special tests running multiple instance of embedded environment excluded by default build. You can run also them with

[source]
---
./ghradlew test --info -PmiTest=true
---


A single test case can be executed using

[source]
----
./gradlew --info test --tests <testname>
----


where <testname> is a fully qualified name of test class
Note you can also use * operator to filter test name

[source]
----

./gradlew --info test --tests *JGroups*
----

You can also run test in debug mode and attach your favorite remote debugger on port 5005 running

[source]
----

./gradlew --info -Dtest.debug
----

Of course you can combine above options for example to debug a single test case.

[[_style]]
== Testing principle and style


Our tests follow the Behavior Driven Development (BDD) technique.
In BDD you focus on specifying the behaviors of a class and write code (tests) that verify that behavior. 

You may be thinking that BDD sounds awfully similar to Test Driven Development (TDD).  In some ways they are similar: they both encourage writing the tests first and to provide full coverage of the  code.
However, TDD doesn't really provide a guide on which kind of tests you should be writing. 

BDD provides you with guidance on how to do testing by focusing on what the behavior of a class is supposed to be.
We introduce BDD to our testing environment by extending the standard JUnit 4.x test framework with BDD capabilities using assertion and mocking frameworks. 

The BDD tests should 

* Clearly define `given-when-then` conditions 
* The method name defines what is expected: f.ex. shouldReturnFalseIfMethodXIsCalledWithNullString() 
* Easy to read the assertions by using http://code.google.com/p/hamcrest/[Hamcrest Matchers]
* Use `given` facts whenever possible to make the test case more readable. It could be the name of the deployed resource adapter, or using the  http://mockito.googlecode.com/svn/branches/1.8.0/javadoc/org/mockito/BDDMockito.html[ BDD Mockito class] to mock the fact. 


We are using two different kind of tests:

* Integration Tests: The goal of these test cases is to validate the whole process of deployment, and interacting with a sub-system by simulating a critical condition. 
* Unit Tests: The goal of these test cases is to stress test some internal behaviour by mocking classes to perfectly reproduce conditions to test. 


[[_integration]]
=== Integration Tests


The integration tests simulate a real condition using particular deployment artifacts packaged as resource adapters.

The resource adapters are created using either the main build environment or by using  http://community.jboss.org/wiki/ShrinkWrap[ShrinkWrap].
Using resource adapters within the test cases will allow you to debug both the resource adapters themself or the JCA container. 

The resource adapters represent the <<given>>
 facts of our BDD tests,  the deployment of the resource adapters represent the <<when>>
 phase,  while the <<then>>
 phase is verified by assertion. 

Note that some tests consider an exception a normal output condition using the JUnit 4.x  `@Exception(expected = "SomeClass.class")` annotation to identify and verify this situation.

This kind of tests run inside our embedded environment and creating deployment descriptor with our builder.
Few custom annotation make embedded environment start/deploy/undeploy/stop in test calsses very simple and human readable.
A test class look like this:

[source]
----

package org.ironjacamar.core.workmanager;

import org.ironjacamar.embedded.Configuration;
import org.ironjacamar.embedded.Deployment;
import org.ironjacamar.embedded.dsl.resourceadapters20.api.ResourceAdaptersDescriptor;
import org.ironjacamar.embedded.junit4.AllChecks;
import org.ironjacamar.embedded.junit4.IronJacamar;
import org.ironjacamar.embedded.junit4.PostCondition;
import org.ironjacamar.embedded.junit4.PreCondition;
import org.ironjacamar.rars.ResourceAdapterFactory;
import org.ironjacamar.rars.wm.WorkConnection;
import org.ironjacamar.rars.wm.WorkConnectionFactory;

import javax.annotation.Resource;

import org.jboss.shrinkwrap.api.spec.ResourceAdapterArchive;

import org.junit.Test;
import org.junit.runner.RunWith;

import static org.junit.Assert.assertNotNull;

/**
 * Basic WorkManager test case
 * @author <a href="mailto:jesper.pedersen@ironjacamar.org">Jesper Pedersen</a>
 */
@RunWith(IronJacamar.class)
@Configuration(full = true)
@PreCondition(condition = AllChecks.class)
@PostCondition(condition = AllChecks.class)
public class WorkManagerTestCase
{
   /** The user transaction */
   @Resource(mappedName = "java:/eis/WorkConnectionFactory")
   private WorkConnectionFactory wcf;

   /**
    * The resource adapter
    * @throws Throwable In case of an error
    */
   @Deployment(order = 1)
   private ResourceAdapterArchive createResourceAdapter() throws Throwable
   {
      return ResourceAdapterFactory.createWorkRar();
   }

   /**
    * The activation
    * @throws Throwable In case of an error
    */
   @Deployment(order = 2)
   private ResourceAdaptersDescriptor createActivation() throws Throwable
   {
      return ResourceAdapterFactory.createWorkDeployment(null);
   }

   /**
    * Deployment
    * @throws Throwable In case of an error
    */
   @Test
   public void testDeployment() throws Throwable
   {
      assertNotNull(wcf);

      WorkConnection wc = wcf.getConnection();
      assertNotNull(wc);

      wc.close();
   }
}

----

While ResourceAdapterFactory methods invoked like this

[source]
----

 /**
    * Create the work.rar
    *
    * @return The resource adapter archive
    */
   public static ResourceAdapterArchive createWorkRar()
   {
      org.jboss.shrinkwrap.descriptor.api.connector16.ConnectorDescriptor raXml = Descriptors
            .create(org.jboss.shrinkwrap.descriptor.api.connector16.ConnectorDescriptor.class, "ra.xml").version("1.6");

      org.jboss.shrinkwrap.descriptor.api.connector16.ResourceadapterType rt = raXml.getOrCreateResourceadapter()
            .resourceadapterClass(WorkResourceAdapter.class.getName());
      org.jboss.shrinkwrap.descriptor.api.connector16.OutboundResourceadapterType ort = rt
            .getOrCreateOutboundResourceadapter().transactionSupport("NoTransaction").reauthenticationSupport(false);
      org.jboss.shrinkwrap.descriptor.api.connector16.ConnectionDefinitionType cdt = ort.createConnectionDefinition()
            .managedconnectionfactoryClass(WorkManagedConnectionFactory.class.getName())
            .connectionfactoryInterface(WorkConnectionFactory.class.getName())
            .connectionfactoryImplClass(WorkConnectionFactoryImpl.class.getName())
            .connectionInterface(WorkConnection.class.getName())
            .connectionImplClass(WorkConnectionImpl.class.getName());

      ResourceAdapterArchive raa = ShrinkWrap.create(ResourceAdapterArchive.class, "work.rar");

      JavaArchive ja = ShrinkWrap.create(JavaArchive.class, "work.jar");
      ja.addPackages(true, WorkConnection.class.getPackage());

      raa.addAsLibrary(ja);
      raa.addAsManifestResource(new StringAsset(raXml.exportAsString()), "ra.xml");

      return raa;
   }

   /**
    * Create the work.rar deployment
    *
    * @param bc The BootstrapContext name; <code>null</code> if default
    * @return The resource adapter descriptor
    */
   public static ResourceAdaptersDescriptor createWorkDeployment(String bc)
   {
      ResourceAdaptersDescriptor dashRaXml = Descriptors.create(ResourceAdaptersDescriptor.class, "work-ra.xml");

      ResourceAdapterType dashRaXmlRt = dashRaXml.createResourceAdapter().archive("work.rar");
      if (bc != null)
         dashRaXmlRt.bootstrapContext(bc);
      ConnectionDefinitionsType dashRaXmlCdst = dashRaXmlRt.getOrCreateConnectionDefinitions();
      org.ironjacamar.embedded.dsl.resourceadapters20.api.ConnectionDefinitionType dashRaXmlCdt = dashRaXmlCdst
            .createConnectionDefinition().className(WorkManagedConnectionFactory.class.getName())
            .jndiName("java:/eis/WorkConnectionFactory").id("WorkConnectionFactory");

      org.ironjacamar.embedded.dsl.resourceadapters20.api.PoolType dashRaXmlPt = dashRaXmlCdt.getOrCreatePool()
            .minPoolSize(0).initialPoolSize(0).maxPoolSize(10);

      return dashRaXml;
   }
   
----



[[_unit]]
=== Unit Tests


We are mocking our input/output conditions in our unit tests using the http://mockito.googlecode.com[Mockito] framework to verify class and method behaviors. 

An example:

[source,java]
----

@Test
public void printFailuresLogShouldReturnNotEmptyStringForWarning() throws Throwable
{
   //given
   RADeployer deployer = new RADeployer();
   File mockedDirectory = mock(File.class);
   given(mockedDirectory.exists()).willReturn(false);

   Failure failure = mock(Failure.class);
   given(failure.getSeverity()).willReturn(Severity.WARNING);

   List failures = Arrays.asList(failure);
   FailureHelper fh = mock(FailureHelper.class);
   given(fh.asText((ResourceBundle) anyObject())).willReturn("myText");
  
   deployer.setArchiveValidationFailOnWarn(true);
  
   //when
   String returnValue = deployer.printFailuresLog(null, mock(Validator.class), 
                                                  failures, mockedDirectory, fh);
  
   //then
   assertThat(returnValue, is("myText"));
}
----


As you can see the BDD style respects the test method name and using the `given-when-then` sequence in order. 

[[_qa]]
== Quality Assurance


In addition to the test suite the IronJacamar project deploys various tools to increase the stability of the project.

The following sections will describe each of these tools.

=== Checkstyle


Checkstyle is a tool that verifies that the formatting of the source code in the project is consistent.

This allows for easier readability and a consistent feel of the project.

The goal is to have zero errors in the report.
The checkstyle report is generated on every build and can be found under each module's build directory.


[[_jacoco]]
=== JaCoCo


JaCoCo generates a test suite matrix for your project which helps you identify where you need additional test coverage.

The reports that the tool provides makes sure that the IronJacamar project has the correct test coverage.

The goal is to have as high code coverage as possible in all areas.
The JaCoco report is generated at every build


The report is generated into

[source]
----

testsuite/build/reports/jacoco
----


The home of JaCoCo is located here: http://www.eclemma.org/jacoco/.

[[_performance]]
== Performance testing


Performance testing can identify areas that need to be improved or completely replaced.

=== JProfiler


Insert the following line in `run.sh` or ``run.bat``:

[source]
----

-agentpath:<path>/jprofiler6/bin/linux-x64/libjprofilerti.so=port=8849
----


where the Java command is executed.

The home of JProfiler is located here: http://www.ej-technologies.com/products/jprofiler/overview.html.

[[_oprofile]]
=== OProfile


OProfile can give a detailed overview of applications running on the machine, including Java program running with OpenJDK.

The home of OProfile is located here: http://oprofile.sourceforge.net.

[[_oprofile_install]]
==== Installation


Enable the Fedora debug repo:

[source]
----

/etc/yum.repos.d/fedora.repo

[fedora-debuginfo]
name=Fedora $releasever - $basearch - Debug
failovermethod=priority
mirrorlist=https://mirrors.fedoraproject.org/metalink?repo=fedora-debug-$releasever&arch=$basearch
enabled=1
gpgcheck=1
gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-fedora-$basearch
----


Install:

[source]
----

dnf install -y oprofile oprofile-jit
dnf install -y yum-plugin-auto-update-debug-info
dnf install -y java-1.6.0-openjdk-debuginfo
----


If you are using Fedora 21 or older, you need to use yum instead of dnf to install OProfile:

[source]
----

yum install -y oprofile oprofile-jit
yum install -y yum-plugin-auto-update-debug-info
yum install -y java-1.6.0-openjdk-debuginfo
----

[[_oprofile_running]]
==== Running


Insert the following line in `run.sh` or ``run.bat``:

[source]
----

-agentpath:/usr/lib64/oprofile/libjvmti_oprofile.so
----


for 64bit JVMs or 

[source]
----

-agentpath:/usr/lib/oprofile/libjvmti_oprofile.so
----


for 32 bit JVMs where the Java command is executed.

Now execute:

[source]
----

opcontrol --no-vmlinux
opcontrol --start-daemon
----


and use the following commands:

[source]
----

opcontrol --start # Starts profiling
opcontrol --dump  # Dumps the profiling data out to the default file
opcontrol --stop  # Stops profiling
----


Once you are done execute:

[source]
----

opcontrol --shutdown  # Shuts the daemon down
----


A report can be generated by:

[source]
----

opreport -l --output-file=<filename>
----


Remember that this is system wide profiling, so make sure that only the services that you want included are running.

More information is available at http://oprofile.sourceforge.net/doc/index.html.

[[_performance_test_suite]]
=== Performance test suite

TODO
